const functions = require("firebase-functions");
const path = require("path");

const {resizeImage} = require("./resizeImage");

const {getAuthorDrafts} = require("./mysql");

const {renderGhostSectionPage} = require("./renderers/section");

const {renderGhostAuthorPage} = require("./renderers/author");

const {
  logUpdate,
  deletePostHtml,
} = require("./renderers/uploader");

const {
  indexPost,
  deleteIndexPost,
} = require("./algolia");

const {
  renderUploadGhostDraft,
  renderUploadGhostPost,
} = require("./renderers/post");

const { 
  log,
  logError
 } = require('./utils');

const {fbstorage} = require("./firebase");

const fbBucketName = functions.config().gcs.fbupload;

// fires on saved draft, triggers preview update
exports.createGhostDraft = functions.https.onRequest(async (req, res) => {
  const {
    body: {
      post: {
        current,
      },
    },
  } = req;

  renderUploadGhostDraft(current);

  logUpdate(current);

  res.status(200).send("draft updated");
});

// fires on saved draft, triggers preview update
exports.updateGhostDraft = functions.https.onRequest(async (req, res) => {
  const {
    body: {
      post: {
        current,
      },
    },
  } = req;

  renderUploadGhostDraft(current);

  res.status(200).send("draft updated");
});

// when post is published in ghost, renders and uploads to new post page
// rerenders the section page for each tag of the post
exports.createGhostPost = functions.https.onRequest(async (req, res) => {
  const {
    body: {
      post: {
        current,
      },
    },
  } = req;

  // render and upload post page
  await renderUploadGhostPost(current);

  // render and upload author page
  await renderGhostAuthorPage(current.primary_author);

  // update section pages for each tag
  current.tags.forEach(async (tag) => {
    renderGhostSectionPage(tag);
  });

  await indexPost(current);

  logUpdate(current);

  res.status(200).send("post created");
});

// when post is published post in ghost is updated, renders and uploads the new post page to google storage
// rerenders and uploads the section page for each tag of the post
exports.updateGhostPost = functions.https.onRequest(async (req, res) => {
  const {
    body: {
      post: {
        current,
      },
    },
  } = req;

  if (current) {
  // render post page
    await renderUploadGhostPost(current);

    await renderGhostAuthorPage(current.primary_author);

    // update section pages with associated tags
    current.tags.forEach((tag) => {
      renderGhostSectionPage(tag);
    });

    await indexPost(current);
  }

  res.status(200).send("doc updated");
});

// when post is unpublished in ghost, post page is deleted from google storage
// rerenders and uploads the section page for each tag of the post - page without the deleted article
exports.deleteGhostPost = functions.https.onRequest(async (req, res) => {
  const {
    body: {
      post: {
        current,
      },
    },
  } = req;

  const {
    slug,
    id,
    tags,
  } = current;

  const path = `${slug}-${id}.html`;

  // delete article in storage
  await deletePostHtml(path);

  // update section pages with associated tags removing the article
  tags.forEach(async (tag) => {
    await renderGhostSectionPage(tag);
  });

  // remove from algolia index
  await deleteIndexPost(id);

  logUpdate(current);

  res.status(200).send("post deleted");
});

// gets drafts and published posts by logged in author
exports.getAuthorPosts = functions.https.onCall(async (data, context) => {
  const {
    auth: {
      token: {
        email,
      },
    },
  } = context;

  const results = await getAuthorDrafts(email);
  const drafts = results.filter((d) => d.status === "draft");
  const published = results.filter((d) => d.status === "published");

  return {
    drafts,
    published,
  };
});

// create resized images when image is uploaded to gcs through ghost
exports.generateSizedImages1 = functions.storage.bucket("static-times-media")
    .object()
    .onFinalize(async (object) => {
      const {
        contentType,
        name,
      } = object;

      const fileExtension = path.extname(name);

      if (!contentType) {
        log("File has no Content-Type, no processing is required");
        return;
      }

      // filter about a second image version auto generated by ghost
      if (name.includes("_o-")) {
        log(`filtered image ${name}`);
        return;
      }

      // filter about a second image version auto generated by ghost
      if (name.includes("-author")) {
        log(`filtered image ${name}`);
        return;
      }


      // image sizes to generate based on template breakpoints
      const imageSizes = [
        {
          name: "312",
          width: 312,
        },
        {
          name: "397",
          width: 397,
        },
        {
          name: "427",
          width: 427,
        },
        {
          name: "574",
          width: 574,
        },
        {
          name: "683",
          width: 683,
        },
        {
          name: "873",
          width: 873,
        },
        {
          name: "1173",
          width: 1173,
        },
        {
          name: "1217",
          width: 1217,
        },

      ];


      // just resize don't keep file type same
      const convertExt = fileExtension;
      // or
      // resize and convert to different file type (webp)
      // const convertExt = 'webp'

      const tasks = [];

      imageSizes.forEach((size) => {
        tasks.push(resizeImage({object, size, convertExt}));
      });
      const results = await Promise.all(tasks);

      const failed = results.some((result) => result.success === false);

      if (failed) {
        logError("Failed execution of extension");
        return;
      } else {
        log("Completed image resizing");
        return;
      }
    });

// fetches html from storage to serve to client
exports.expressHandler = functions.https.onRequest(async (req, res) => {
  const fileName = req.path === "/" ? "front-page.html" : req.path.replace("/", "");

  const file = fbstorage.bucket(fbBucketName).file(fileName);
  const rs = file.createReadStream();

  const t0 = Date.now();
  res.set({
    "Cache-Control": "max-age=0, s-maxage=31536000",
    "Content-Type": "text/html; charset=utf-8",
  });
  rs.pipe(res);
  rs.on("finish", () => {
    log("file stream took", Date.now() - t0, "ms");
  });
  rs.on("error", (err) => {
    logError(err);
    res.status(404).send("Not Found");
  });
});
